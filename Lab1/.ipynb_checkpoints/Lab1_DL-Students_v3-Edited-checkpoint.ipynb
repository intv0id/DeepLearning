{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning  Lab Session </h1>\n",
    "<h1 style=\"text-align:center\">First Lab Session - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\">Artificial Neural Networks for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Student 1:</b> SOGNO Martin\n",
    "<br>\n",
    "<b> Student 2:</b> TRASSOUDAINE Clément\n",
    "<br>\n",
    "<b> Group name:</b> deeplearn25\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Artificial Neural Networks. Answers and experiments should be made by groups of two students. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "To generate your final report and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed ans submitted by April 13th 2018 (23:59:59 CET). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this lab session, you will implement, train and test a Neural Network\n",
    "for the Handwritten Digits Recognition problem <a href=\"http://yann.lecun.com/exdb/mnist/\"> [1] </a> with  different settings of hyperparameters. You will use the MNIST dataset which was constructed from scanned documents available from the National Institute of Standards and Technology (NIST). Images of digits were taken from a variety of scanned documents, normalized in size and centered. \n",
    "\n",
    "\n",
    "<img src=\"Nimages/mnist.png\",width=\"350\" height=\"500\" align=\"center\">\n",
    "<center><span>Figure 1: MNIST digits examples</span></center>\n",
    "\n",
    "\n",
    "This assignment includes a written part of programms to help you understand how to build and train\n",
    "your neural net and then to test your code and get results. \n",
    "\n",
    "1. <a href=\"NeuralNetwork.py\"> NeuralNetwork.py </a> \n",
    "2. <a href=\"transfer_functions.py\"> transfer_functions.py </a> \n",
    "3.  <a href=\"utils.py \"> utils.py </a> \n",
    "\n",
    "\n",
    "Functions defined inside the python files mentionned above can be imported  using the python command \"from filename import function\".\n",
    "\n",
    "You will use the following libraries:\n",
    "\n",
    "1. <a href=\"http://cs231n.github.io/python-numpy-tutorial/\"> numpy </a>: for creating arrays and using methods to manipulate arrays;\n",
    "\n",
    "2. <a href=\"http://matplotlib.org/\"> matplotlib  </a>: for making plots.\n",
    "\n",
    "Before starting the lab, please launch the cell below. After that, you may not need to do any imports during the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "from transfer_functions import *\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 :  Your First Neural Network\n",
    "\n",
    "<b>Part 1</b>: Before designing and writing your code, you will first work on a neural network by hand. \n",
    "Consider the following neural network with two inputs $x=(x_1,x_2)$, one hidden layer and a single output unit $y$.\n",
    "The initial weights are set to random values. Neurons 6 and 7 represent biases. Bias values are equal to 1. You will consider a training sample whose feature vector is $x = (0.8, 0.2)$ and whose label is $y = 0.4$.\n",
    "\n",
    "Assume that neurons have a sigmoid activation function  $f(x)=\\frac{1}{(1+e^{-x})}$. The loss function $L$ is a Mean Squared Error (MSE): if $o$ denotes the output of the neural network, then the loss for a given sample $(o, y)$ is $L(o, y) = \\left|\\left| o - y \\right|\\right|^2$. In the following, you will assume that if you want to backpropagate the error on a whole batch, you will backpropagate the average error on that batch. More formally, let $((x^{(1)}, y^{(1)}), ..., (x^{(N)}, y^{(N)}))$ be a batch and $o^{(k)}$ the output associated to $x^{(k)}$. Then the total error $\\bar{L}$ will be as follows:\n",
    "\n",
    "<div align=\"center\">$\\bar{L} = \\frac{1}{N} \\sum_{k=1}^{N} L(o^{(k)}, y^{(k)})$.</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Nimages/NN.png\", width=\"700\" height=\"900\"> \n",
    "<center><span>Figure 2: Neural network </span></center>\n",
    "\n",
    "\n",
    "<b>Question 1.1.1</b>: Compute the new values of weights $w_{i,j}$ after a forward pass and a backward pass, and the outputs of the neural network before and after the backward path, when the learning rate is $\\lambda$=5.\n",
    "$w_{i,j}$ is the weight of the connexion between neuron $i$ and neuron $j$. Please detail your computations in the cell below and print your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD PASS 1 ===\n",
      "Output = 0.5597295991095776\n",
      "=== BACKWARD PASS ===\n",
      "w13 = 0.320017024273193\n",
      "w14 = -0.5144154043054077\n",
      "w23 = 0.8050042560682983\n",
      "w24 = 0.1963961489236481\n",
      "w63 = 0.22502128034149133\n",
      "w64 = -0.4180192553817596\n",
      "w35 = -0.7193207522517233\n",
      "w45 = 0.34111245902831805\n",
      "w75 = 0.3151946304850383\n",
      "=== FORWARD PASS 2 ===\n",
      "Output = 0.48759020878680587\n"
     ]
    }
   ],
   "source": [
    "lr = 5.0\n",
    "x1, x2, x6, x7 = 0.8, 0.2, 1.0, 1.0\n",
    "w13, w14, w23, w24, w63, w64 = 0.3, -0.5, 0.8, 0.2, 0.2, -0.4\n",
    "w35, w45, w75 = -0.6, 0.4, 0.5\n",
    "y = 0.4\n",
    "\n",
    "# Compute the output of each neuron\n",
    "x3 = sigmoid(w13*x1 + w23*x2 + w63*x6)\n",
    "x4 = sigmoid(w14*x1 + w24*x2 + w64*x6)\n",
    "x5 = sigmoid(w35*x3 + w45*x4 + w75*x7)\n",
    "\n",
    "print(\"=== FORWARD PASS 1 ===\")\n",
    "print(\"Output =\", x5)\n",
    "\n",
    "# Compute the output error\n",
    "e5 = -(y-x5)\n",
    "\n",
    "# Compute the derivative for the output weights\n",
    "d_w35 = e5*dsigmoid(x5)*x3\n",
    "d_w45 = e5*dsigmoid(x5)*x4\n",
    "d_w75 = e5*dsigmoid(x5)*x7 \n",
    "\n",
    "# Compute the errors for layer 1\n",
    "e3 = w35*d_w35/x3\n",
    "e4 = w45*d_w45/x4\n",
    "\n",
    "# Compute the derivative for the input weights\n",
    "d_w13 = e3*dsigmoid(x3)*x1\n",
    "d_w14 = e4*dsigmoid(x4)*x1\n",
    "d_w23 = e3*dsigmoid(x3)*x2\n",
    "d_w24 = e4*dsigmoid(x4)*x2\n",
    "d_w63 = e3*dsigmoid(x3)*x6\n",
    "d_w64 = e4*dsigmoid(x4)*x6\n",
    "\n",
    "# Update weights\n",
    "w13 -= lr*d_w13\n",
    "w14 -= lr*d_w14\n",
    "w23 -= lr*d_w23\n",
    "w24 -= lr*d_w24\n",
    "w63 -= lr*d_w63\n",
    "w64 -= lr*d_w64\n",
    "w35 -= lr*d_w35\n",
    "w45 -= lr*d_w45\n",
    "w75 -= lr*d_w75\n",
    "\n",
    "print(\"=== BACKWARD PASS ===\")\n",
    "print(\"w13 =\", w13)\n",
    "print(\"w14 =\", w14)\n",
    "print(\"w23 =\", w23)\n",
    "print(\"w24 =\", w24)\n",
    "print(\"w63 =\", w63)\n",
    "print(\"w64 =\", w64)\n",
    "print(\"w35 =\", w35)\n",
    "print(\"w45 =\", w45)\n",
    "print(\"w75 =\", w75)\n",
    "\n",
    "# Update the outputs\n",
    "x3 = sigmoid(w13*x1 + w23*x2 + w63*x6)\n",
    "x4 = sigmoid(w14*x1 + w24*x2 + w64*x6)\n",
    "x5 = sigmoid(w35*x3 + w45*x4 + w75*x7)\n",
    "\n",
    "print(\"=== FORWARD PASS 2 ===\")\n",
    "print(\"Output =\", x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 2</b>: Neural Network Implementation\n",
    "\n",
    "Please read all source files carefully and understand the data structures and all functions.\n",
    "You are to complete the missing code. \n",
    "First you should define the neural network (using the NeuralNetwork class, see in the <a href=\"NeuralNetwork.py\"> NeuralNetwork.py</a> file) and reinitialise weights. \n",
    "Then you will need to complete the feedforward() and the backpropagate() functions. \n",
    "\n",
    "<b>Question 1.2.1</b>: Implement the feedforward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"\n",
    "            Implements the feed-forward propagation\n",
    "        \"\"\"\n",
    "        bias = np.array([[1.]])\n",
    "        self.o_input = np.array(inputs).reshape((-1, 1))\n",
    "        \n",
    "        # Compute the hidden layer\n",
    "        self.u_hidden = np.dot(\n",
    "            self.W_input_to_hidden.T,\n",
    "            np.vstack((self.o_input, bias))            \n",
    "        )\n",
    "        self.o_hidden = self.transfer_f(self.u_hidden)\n",
    "        \n",
    "        # Compute the output layer\n",
    "        self.u_output = np.dot(\n",
    "            self.W_hidden_to_output.T,\n",
    "            np.vstack((self.o_hidden, bias))\n",
    "        )\n",
    "        self.o_output = self.transfer_f(self.u_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.2.2</b>: Test your implementation: create the Neural Network defined in Part 1 and see if the feedforward() function you implemented gives the same results as the ones you found by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output = 0.5597295991095776\n"
     ]
    }
   ],
   "source": [
    "# First define your neural network\n",
    "model = NeuralNetwork(input_layer_size=2, \n",
    "                      hidden_layer_size=2,\n",
    "                      output_layer_size=1, \n",
    "                      transfer_f=sigmoid, \n",
    "                      transfer_df=dsigmoid)\n",
    "\n",
    "# Then initialize the weights according to Figure 2\n",
    "W_input_to_hidden = np.array([[.3, -.5],\n",
    "                              [.8,  .2],\n",
    "                              [.2, -.4]])\n",
    "W_hidden_to_output = np.array([[-.6],\n",
    "                               [ .4],\n",
    "                               [ .5]])\n",
    "model.weights_init(W_input_to_hidden, W_hidden_to_output)\n",
    "\n",
    "# Feed test values\n",
    "test = [[0.8, 0.2]]\n",
    "model.feedforward(test)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output =\", model.o_output[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.2.3</b>: Implement the backpropagate() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def backpropagate(self, targets, learning_rate=5.0):\n",
    "        \"\"\"\n",
    "            Implements backward propagation\n",
    "            \n",
    "            Returns the error\n",
    "        \"\"\"\n",
    "        targets = np.array(targets).reshape((-1, 1))\n",
    "        bias = np.array([[1.]])\n",
    "        \n",
    "        # Calculate errors\n",
    "        self.dE_du_output = np.multiply(\n",
    "            self.o_output-targets,\n",
    "            self.transfer_df(self.o_output)\n",
    "        )\n",
    "        self.dE_du_hidden = np.multiply(\n",
    "            np.dot(self.W_hidden_to_output, self.dE_du_output),\n",
    "            self.transfer_df(np.vstack((self.o_hidden, bias)))\n",
    "        )[:-1,:] #Take care of removing the bias\n",
    "        \n",
    "        # update weights\n",
    "        self.W_hidden_to_output -= learning_rate*np.dot(np.vstack((self.o_hidden, bias)), \n",
    "                                                        self.dE_du_output.T)\n",
    "        self.W_input_to_hidden -= learning_rate*np.dot(np.vstack((self.o_input, bias)), \n",
    "                                                       self.dE_du_hidden.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.2.4</b>: Test your implementation: create the Neural Network defined in Part 1 and see if the backpropagate() function you implemented gives the same weight updates as the ones you found by hand. Do another forward pass and see if the new output is the same as the one you obtained in Question 1.1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_input_to_hidden = [[ 0.32001702 -0.5144154 ]\n",
      " [ 0.80500426  0.19639615]\n",
      " [ 0.22502128 -0.41801926]]\n",
      "W_hidden_to_output = [[-0.71932075]\n",
      " [ 0.34111246]\n",
      " [ 0.31519463]]\n",
      "Output = [[0.48759021]]\n"
     ]
    }
   ],
   "source": [
    "# First define your neural network\n",
    "model = NeuralNetwork(input_layer_size=2, \n",
    "                      hidden_layer_size=2,\n",
    "                      output_layer_size=1, \n",
    "                      transfer_f=sigmoid, \n",
    "                      transfer_df=dsigmoid)\n",
    "\n",
    "# Then initialize the weights according to Figure 2\n",
    "W_input_to_hidden = np.array([[.3, -.5],\n",
    "                              [.8,  .2],\n",
    "                              [.2, -.4]])\n",
    "W_hidden_to_output = np.array([[-.6],\n",
    "                               [ .4],\n",
    "                               [ .5]])\n",
    "model.weights_init(W_input_to_hidden, W_hidden_to_output)\n",
    "\n",
    "# Feed test values\n",
    "test = [0.8, 0.2]\n",
    "\n",
    "model.feedforward(test)\n",
    "\n",
    "# Backpropagate\n",
    "targets = [[0.4]]\n",
    "model.backpropagate(targets)\n",
    "\n",
    "# Print weights\n",
    "print(\"W_input_to_hidden =\",  model.W_input_to_hidden)\n",
    "print(\"W_hidden_to_output =\", model.W_hidden_to_output)\n",
    "\n",
    "# Feed test values again\n",
    "model.feedforward(test)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output =\", model.o_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked your implementations and found that everything was fine? Congratulations! You can move to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Handwritten Digits Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset consists of handwritten digit images. It is split into a training set containing 60,000 samples and a test set containing 10,000 samples. In this Lab Session, the official training set of 60,000 images is divided into an actual training set of 50,000 samples a validation set of 10,000 samples. All digit images have been size-normalized and centered in a fixed size image of 28 x 28 pixels. Images are stored in byte form: you will use the NumPy python library to convert data files into NumPy arrays that you will use to train your Neural Networks.\n",
    "\n",
    "You will first work with a small subset of MNIST (1000 samples), then on a very small subset of MNIST (10 samples), and eventually run a model on the whole one.\n",
    "\n",
    "The MNIST dataset is available in the Data folder.\n",
    "To get the training, testing and validation data, run the load_data() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data .....\n"
     ]
    }
   ],
   "source": [
    "# Just run that cell ;-)\n",
    "training_data, validation_data, test_data = load_data()\n",
    "small_training_data = (training_data[0][:1000], training_data[1][:1000])\n",
    "small_validation_data = (validation_data[0][:200], validation_data[1][:200])\n",
    "indices = [1, 3, 5, 7, 2, 0, 13, 15, 17, 4]\n",
    "vsmall_training_data = ([training_data[0][i] for i in indices], [training_data[1][i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And you can run that cell if you want to see what the MNIST dataset looks like\n",
    "ROW = 2\n",
    "COLUMN = 5\n",
    "for i in range(ROW * COLUMN):\n",
    "    # train[i][0] is i-th image data with size 28x28\n",
    "    image = np.array(training_data[0][i]).reshape(28, 28)   \n",
    "    plt.subplot(ROW, COLUMN, i+1)          \n",
    "    plt.imshow(image, cmap='gray')  # cmap='gray' is for black and white picture.\n",
    "plt.axis('off')  # do not show axis value\n",
    "plt.tight_layout()   # automatic padding between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 1</b>: Build a bigger Neural Network\n",
    "\n",
    "The input layer of the neural network that you will build contains neurons encoding the values of the input pixels. The training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits. Thus, the input layer contains 784=28×28 units. The second layer of the network is a hidden layer. We set the number of neurons in the hidden layer to 30. The output layer contains 10 neurons.\n",
    "\n",
    "<b>Question 2.1.1</b>: Create the network described above using the NeuralNetwork class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=30,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1.2</b>: Train your Neural Network on the small subset of MNIST (300 iterations) and print the new accuracy on test data. You will use small_validation_data for validation. Try different learning rates (0.1, 1.0, 10.0). You should use the train() function of the NeuralNetwork class to train your network, and the weights_init() function to reinitialize weights between tests. Print the accuracy of each model on test data using the predict() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train NN and print accuracy on test data\n",
    "\n",
    "from time import time\n",
    "\n",
    "def accuracy(model, validation_data):\n",
    "    l = len(validation_data[0])\n",
    "    def isCorrect_iter():\n",
    "        for i in range(l):\n",
    "            #print(len(validation_data[0][i]))\n",
    "            model.feedforward(validation_data[0][i])\n",
    "            yield (\n",
    "                np.argmax(validation_data[1][i])\n",
    "                == \n",
    "                np.argmax(model.o_output\n",
    "                     .reshape(-1)\n",
    "                     .tolist())\n",
    "            )\n",
    "            \n",
    "    return sum(isCorrect_iter())/l\n",
    "\n",
    "def train_and_test(model, training_data, validation_data, learning_rate, iterations):\n",
    "    model.weights_init()\n",
    "    t0 = time()\n",
    "    for j in range(1, iterations+1):\n",
    "        if j%50 == 0:\n",
    "            t1 = time()\n",
    "            print(\"Iteration {}/{}\\t-- Elapsed time: {}s\\t-- Remaining time: {}s.\"\n",
    "                      .format(j,iterations, int(t1-t0), int((iterations-j)*(t1-t0)/j))\n",
    "            )\n",
    "        for i in range(len(training_data[0])):\n",
    "            model.feedforward([training_data[0][i]])\n",
    "            model.backpropagate([training_data[1][i]],\n",
    "                                learning_rate=0.1)\n",
    "    return accuracy(mnist_model, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate 0.1, 1 and 10\n",
    "for lr in [0.1, 1, 10]:\n",
    "    acc = train_and_test(model=mnist_model, \n",
    "                         training_data=small_training_data, \n",
    "                         validation_data=validation_data, \n",
    "                         learning_rate=lr, \n",
    "                         iterations=300)\n",
    "    \n",
    "\n",
    "    print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "              .format(lr, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1.3</b>: Do the same with 15 and 75 hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network\n",
    "# 15 hidden neurons \n",
    "# Learning rate 0.1, 1, 10\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=15,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)\n",
    "\n",
    "for lr in [0.1, 1, 10]:\n",
    "    acc = train_and_test(model=mnist_model, \n",
    "                         training_data=small_training_data, \n",
    "                         validation_data=validation_data, \n",
    "                         learning_rate=lr, \n",
    "                         iterations=300)\n",
    "\n",
    "    print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "              .format(lr, acc*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75 hidden neurons\n",
    "# Learning rate 0.1, 1, 10\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=75,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)\n",
    "\n",
    "for lr in [0.1, 1, 10]:\n",
    "    acc = train_and_test(model=mnist_model, \n",
    "                         training_data=small_training_data, \n",
    "                         validation_data=validation_data, \n",
    "                         learning_rate=lr, \n",
    "                         iterations=300)\n",
    "\n",
    "    print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "              .format(lr, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1.3</b>: Repeat Questions 2.1.2 and 2.1.3 on the very small datasets. You will use small_validation_data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN and print accuracy on test data\n",
    "# 30 hidden neurons\n",
    "# Learning rate 0.1, 1, 10\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=30,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)\n",
    "\n",
    "for lr in [0.1, 1, 10]:\n",
    "    acc = train_and_test(model=mnist_model, \n",
    "                         training_data=vsmall_training_data, \n",
    "                         validation_data=small_validation_data, \n",
    "                         learning_rate=lr, \n",
    "                         iterations=300)\n",
    "\n",
    "    print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "              .format(lr, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 hidden neurons\n",
    "# Learning rate 0.1, 1, 10\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=15,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)\n",
    "\n",
    "for lr in [0.1, 1, 10]:\n",
    "    acc = train_and_test(model=mnist_model, \n",
    "                         training_data=vsmall_training_data, \n",
    "                         validation_data=small_validation_data, \n",
    "                         learning_rate=lr, \n",
    "                         iterations=300)\n",
    "\n",
    "    print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "              .format(lr, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75 hidden neurons\n",
    "# Learning rate 0.1, 1, 10\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=75,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)\n",
    "\n",
    "for lr in [0.1, 1, 10]:\n",
    "    acc = train_and_test(model=mnist_model, \n",
    "                         training_data=vsmall_training_data, \n",
    "                         validation_data=small_validation_data, \n",
    "                         learning_rate=lr, \n",
    "                         iterations=300)\n",
    "\n",
    "    print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "              .format(lr, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1.5</b>: Explain the results you obtained at Questions 2.1.2, 2.1.3 and 2.1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1.6</b>: Among all the numbers of hidden neurons and learning rates you tried in previous questions, which ones would you expect to achieve best performances on the whole dataset? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1.7</b>: Train a model with the number of hidden neurons and the learning rate you chose in Question 2.1.6 and print its accuracy on the test set. You will use validation_data for validation. Training can be long on the whole dataset (~40 minutes): we suggest that you work on the optional part while waiting for the training to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75 hidden neurons, learning rate at 1\n",
    "mnist_model = NeuralNetwork(input_layer_size=784, \n",
    "                            hidden_layer_size=75,\n",
    "                            output_layer_size=10, \n",
    "                            transfer_f=sigmoid, \n",
    "                            transfer_df=dsigmoid)\n",
    "\n",
    "lr = 1\n",
    "acc = train_and_test(model=mnist_model, \n",
    "                     training_data=training_data, \n",
    "                     validation_data=validation_data, \n",
    "                     learning_rate=lr, \n",
    "                     iterations=300)\n",
    "\n",
    "print(\"\\nThe accuracy for a learning rate of {} is {:.2f}%.\\n\"\n",
    "          .format(lr, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 2 (optional)</b>: Another loss function\n",
    "\n",
    "In classification problems, we usually replace the sigmoids in the output layer by a \"softmax\" function and the MSE loss by a \"cross-entropy\" loss. More formally, let $u = (u_1, ..., u_n)$ be the vector representing the activation of the output layer of a Neural Network. The output of that neural network is $o = (o_1, ..., o_n) = \\textrm{softmax}(u)$, and\n",
    "\n",
    "<div align=\"center\">$\\textrm{softmax}(u) = (\\frac{e^{u_1}}{\\sum_{k=1}^n e^{u_k}}, ..., \\frac{e^{u_n}}{\\sum_{k=1}^n e^{u_k}})$.</div>\n",
    "\n",
    "If $t = (t_1, ..., t_n)$ is a vector of non-negative targets such that $\\sum_{k=1}^n t_k = 1$ (which is the case in classification problems, where one target is equal to 1 and all others are equal to 0), then the cross-entropy loss is defined as follows:\n",
    "\n",
    "<div align=\"center\">$L_{xe}(o, t) = - \\sum_{k=1}^n t_k\\log(o_k)$.</div>\n",
    "\n",
    "<b>Question 2.2.1</b>: Let $L_{xe}$ be the cross-entropy loss function and $u_i$, $i \\in \\lbrace 1, ..., n \\rbrace$, be the activations of the output neurons. Let us assume that the transfer function of the output neurons is the softmax function. Targets are $t_1, ..., t_n$. Derive a formula for $\\frac{\\partial L_{xe}}{\\partial u_i}$ (details of your calculations are not required)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: $\\frac{\\partial L_{xe}}{\\partial u_i} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.2.2</b>: Implement a new feedforward() function and a new backpropagate() function adapted to the cross-entropy loss instead of the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def feedforward_xe(self, inputs):\n",
    "        pass\n",
    "\n",
    "    def backpropagate_xe(self, targets, learning_rate=5.0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.2.3</b>: Create a new Neural Network with the same architecture as in Question 2.1.1 and train it using the softmax cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network\n",
    "mnist_model_xe = \n",
    "\n",
    "# Train NN and print accuracy on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.2.4</b>: Compare your results with the MSE loss and with the cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><b>THE END!</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
